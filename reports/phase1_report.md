# Phase 1 开发报告：物理环境层（CPD 区块链博弈环境）

> **项目**：基于 LLM 代理的区块链激励兼容性仿真平台  
> **阶段**：Phase 1 — 物理环境层  
> **日期**：2026-02-11  
> **状态**：✅ 已完成

---

## 一、开发目标

根据项目说明书（`plan.md`）第三节"四层架构开发指南"中的第 1 层要求，本阶段的核心任务是：

1. 构建基于 Gymnasium 的 **CPD（建设-寄生-破坏）** 区块链共识博弈环境
2. 实现 **三维连续动作空间** 及 **单纯形归一化** 约束
3. 实现论文设计的 **非线性耦合效用函数**，确保"过度破坏反噬寄生收益"的博弈机制
4. 保持物理环境与 LLM 调用逻辑的 **模块化解耦**

---

## 二、项目结构

本阶段创建的文件结构如下：

```
Gymnasium/
├── gymnasium/envs/blockchain/           # 物理环境层（Gymnasium 原生环境）
│   ├── __init__.py                      # 包声明
│   └── cpd_env.py                       # ★ CPD 博弈环境核心实现
├── blockchain_sim/                      # LLM 代理模块（独立于 Gym，后续阶段开发）
│   └── __init__.py                      # 包声明，预留模块接口说明
├── gymnasium/envs/__init__.py           # [修改] 新增环境注册项
└── reports/
    └── phase1_report.md                 # 本报告
```

**设计原则**：物理环境层（`gymnasium/envs/blockchain/`）与 LLM 代理逻辑（`blockchain_sim/`）彻底分离，遵循说明书第五节"模块化解耦"的要求。

---

## 三、核心实现详解

### 3.1 动作空间：CPD 三维连续单纯形

| 维度 | 符号 | 含义 | 取值范围 |
|------|------|------|---------|
| 0 | `c` | 诚实建设（Constructive） | [0, 1] |
| 1 | `p` | 寄生挖掘（Parasitic） | [0, 1] |
| 2 | `d` | 攻击破坏（Destructive） | [0, 1] |

**约束**：`c + p + d = 1`（单纯形归一化）

```python
# 归一化实现（cpd_env.py 中的静态方法）
@staticmethod
def _simplex_normalize(action: np.ndarray) -> np.ndarray:
    action = np.maximum(action, 0.0)
    total = action.sum()
    if total < 1e-8:
        return np.array([1.0, 0.0, 0.0])  # 退化为纯诚实挖矿
    return action / total
```

相较于 SquirRL 的离散动作集合（Wait, Match 等），本环境将动作空间提升为三维连续资源分配向量，支持更细粒度的博弈策略表达。

### 3.2 观测空间：6 维状态向量

| 索引 | 字段名 | 含义 | 范围 |
|------|--------|------|------|
| 0 | `cumulative_reward` | 代理的累计收益 | (-∞, +∞) |
| 1 | `opponent_efficiency` | 对手平均挖矿效率 η_j | [0, 1] |
| 2 | `self_hash_share` | 自身算力份额 α_i | [0, 1] |
| 3 | `round_progress` | 当前轮次/总轮次（归一化进度） | [0, 1] |
| 4 | `last_reward` | 上一轮获得的即时奖励 | (-∞, +∞) |
| 5 | `efficiency_delta` | 对手效率变化量（本轮 - 上轮） | (-∞, +∞) |

观测空间的设计为后续 Phase 2 的语义感知层（`translator.py`）提供了清晰的映射对象——每个维度都对应一个可翻译为自然语言的经济学概念。

### 3.3 非线性耦合效用函数

本环境的核心效用函数为：

$$U_i = R \cdot \alpha_i \cdot c_i + R \cdot p_i \cdot \eta_j^{\beta} - \lambda \cdot d_i^2$$

其中：
- **$R$** = 基础区块奖励（默认 10.0）
- **$\alpha_i$** = 矿工 $i$ 的算力份额
- **$\eta_j$** = 对手的平均挖矿效率（耦合项）
- **$\beta$** = 寄生回报的凸性参数（默认 1.5），$\beta > 1$ 使寄生收益对对手效率的敏感度增强
- **$\lambda$** = 破坏行为的成本系数（默认 2.0），二次项使边际成本递增

**耦合机制**：效率更新公式

$$\eta_j^{(t+1)} = \text{clip}\left(\eta_j^{(t)} - \kappa \cdot \sum_{k \neq j} d_k + \text{recovery},\ \eta_{\min},\ 1.0\right)$$

- **$\kappa$** = 破坏对效率的影响因子（默认 0.3）
- **recovery** = 每轮自然恢复量（默认 0.05）
- **$\eta_{\min}$** = 效率下限（默认 0.1），防止效率降为零

**关键博弈洞察**：由于寄生收益 $R \cdot p_i \cdot \eta_j^{\beta}$ 与对手效率 $\eta_j$ 正相关，而过度破坏（高 $d_i$）会降低 $\eta_j$，因此 **破坏行为会间接削弱自身的寄生收益** ——这正是论文设计中"非线性耦合反噬"的数学表达。

### 3.4 对手策略

| 策略名 | 行为模式 | 注册环境 ID |
|--------|---------|------------|
| `honest` | 纯诚实挖矿 `[1,0,0]` | `BlockchainCPD-v0` |
| `random` | Dirichlet 随机分配 | `BlockchainCPD-v0-Random` |
| `tit_for_tat` | 模仿代理上一轮动作 | `BlockchainCPD-v0-TFT` |

### 3.5 可配置参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `num_miners` | 2 | 矿工数量（支持多代理） |
| `max_rounds` | 100 | 每局最大轮次 |
| `base_reward` | 10.0 | 基础区块奖励 R |
| `alpha` | 均分 | 算力分布（可在 reset 时通过 options 覆盖） |
| `beta` | 1.5 | 寄生回报凸性参数 |
| `lambda_` | 2.0 | 破坏行为成本系数 |
| `kappa` | 0.3 | 破坏对效率的影响因子 |
| `eta_min` | 0.1 | 效率下限 |
| `eta_recovery` | 0.05 | 每轮效率自然恢复量 |
| `agent_id` | 0 | 受控代理的矿工索引 |
| `opponent_policy` | "honest" | 对手策略 |

---

## 四、验证测试结果

### 4.1 基础功能测试

| 测试项 | 结果 |
|--------|------|
| 环境创建（`gym.make`） | ✅ 通过 |
| reset 初始化 | ✅ obs=[0, 1, 0.5, 0, 0, 0]，符合预期 |
| 诚实挖矿 step | ✅ reward=5.0（R·α_i·c_i = 10·0.5·1 = 5） |
| 寄生挖矿 step | ✅ reward=7.98，高于纯诚实 |
| 非归一化动作处理 | ✅ [3,2,5] → 自动归一化为 [0.3,0.2,0.5] |
| 完整 episode 运行 | ✅ 100 轮正常终止 |
| ANSI 渲染 | ✅ 文本输出正常 |
| 随机对手 | ✅ 正常运行 |

### 4.2 博弈耦合特性验证

在 50 轮 episode 中，不同策略的总收益对比：

| 策略 | c | p | d | 总收益 | 对手最终 η |
|------|---|---|---|--------|-----------|
| 纯诚实 | 1.0 | 0.0 | 0.0 | **250.00** | 1.000 |
| 轻度寄生 | 0.6 | 0.3 | 0.1 | **299.00** | 1.000 |
| 重度寄生 | 0.2 | 0.7 | 0.1 | **399.00** | 1.000 |
| 均衡分配 | 0.34 | 0.33 | 0.33 | **105.89** | 0.100 |
| 重度破坏 | 0.1 | 0.3 | 0.6 | **3.78** | 0.100 |
| 纯破坏 | 0.0 | 0.0 | 1.0 | **-100.00** | 0.100 |

**关键发现**：
- 重度寄生（低 d）取得最高收益，因为对手效率保持完好
- 当 d 增大到 0.33 以上，对手效率迅速崩溃至下限 0.1，导致寄生收益几乎归零
- 纯破坏策略严重亏损（-100），验证了"反噬"机制

### 4.3 动态耦合验证（以牙还牙对手）

前 10 轮重度破坏（d=0.7），后 10 轮切换为寄生（p=0.8）：

| 轮次 | 策略 | 即时奖励 | 对手效率 η |
|------|------|---------|-----------|
| 1 | 破坏 | 1.52 | 0.840 |
| 5 | 破坏 | -0.05 | 0.200 |
| 10 | 破坏 | -0.42 | 0.100 |
| 11 | 寄生 | 0.73 | 0.120 |
| 15 | 寄生 | 1.09 | 0.200 |
| 20 | 寄生 | 1.67 | 0.300 |

**解读**：前 10 轮的破坏将对手效率从 1.0 压至 0.1，导致第 11 轮切换寄生时收益仅为 0.73（远低于效率完好时的约 8.0）。之后效率以 recovery=0.05 缓慢恢复，寄生收益逐步提升——但恢复速度远慢于破坏速度，量化体现了"过度破坏的长期代价"。

---

## 五、与 SquirRL 的对比

| 维度 | SquirRL (NDSS 2021) | 本项目 Phase 1 |
|------|---------------------|---------------|
| 动作空间 | 离散集合（Wait, Adopt, Match 等） | 三维连续单纯形 [c, p, d] |
| 策略表达 | 有限离散策略 | 连续资源分配梯度 |
| 奖励机制 | 区块奖励 + 叔块奖励 | 非线性耦合效用函数（含反噬） |
| 对手建模 | 固定策略 | 多种策略（honest/random/tit_for_tat） |
| 决策范式 | DRL（追求数学最优） | 为 LLM 有限理性设计（Phase 4 接入） |
| 可解释性 | 黑盒神经网络 | 为 CoT 白盒推理预留接口（Phase 2-4） |

---

## 六、后续开发路线

| 阶段 | 内容 | 核心文件 | 状态 |
|------|------|---------|------|
| ~~Phase 1~~ | ~~物理环境层~~ | ~~cpd_env.py~~ | ✅ 完成 |
| Phase 2 | 语义感知层：obs → 自然语言 | `blockchain_sim/translator.py` | 待开发 |
| Phase 3 | 执行控制层：JSON Schema + 归一化容错 | `blockchain_sim/executor.py` | 待开发 |
| Phase 4 | 认知决策层：LLM API + CoT + 双层记忆 | `blockchain_sim/cognition.py`, `memory.py` | 待开发 |
| Phase 5 | 集成实验：RQ1/RQ2/RQ3 | `blockchain_sim/runner.py` | 待开发 |

---

## 七、环境使用示例

```python
import gymnasium as gym
import numpy as np

# 创建环境
env = gym.make("BlockchainCPD-v0", num_miners=2, max_rounds=100)

# 初始化
obs, info = env.reset(seed=42)

# 博弈循环
for step in range(100):
    # 代理决策：分配资源到 [建设, 寄生, 破坏]
    action = np.array([0.6, 0.3, 0.1])
    
    obs, reward, terminated, truncated, info = env.step(action)
    
    if terminated:
        break

# 获取博弈历史（供记忆模块使用）
history = env.unwrapped.get_history()
```
