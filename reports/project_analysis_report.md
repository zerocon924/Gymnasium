# 生成式认知驱动的区块链共识安全分析框架 — 项目分析报告

## 一、项目总体架构

本项目基于 OpenAI Gymnasium 框架构建了一个 **LLM 驱动的区块链激励兼容性仿真平台**。核心思想是：用大语言模型（LLM）模拟矿工在复杂激励机制下的博弈行为，重点关注 **有限理性**（Bounded Rationality）下的博弈均衡演化，而非传统深度强化学习（DRL）的全局最优解。

### 1.1 四层架构总览

```
┌─────────────────────────────────────────────────────────┐
│                    runner.py（实验编排层）                 │
│          RQ1 / RQ2 / RQ3 实验的生命周期管理                │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐          │
│  │translator│───→│cognition │───→│ executor │          │
│  │  .py     │    │  .py     │    │  .py     │          │
│  │语义感知层│    │认知决策层│    │执行控制层│          │
│  └──────────┘    └──────────┘    └──────────┘          │
│       ↑              ↕                │                 │
│       │         ┌──────────┐          │                 │
│       │         │ memory   │          │                 │
│       │         │  .py     │          │                 │
│       │         │双层记忆  │          │                 │
│       │         └──────────┘          │                 │
│       │                               ↓                 │
│  ┌─────────────────────────────────────────────────┐    │
│  │         cpd_env.py（Gymnasium 物理环境层）        │    │
│  │    CPD 三维连续动作空间 + 非线性耦合效用函数       │    │
│  └─────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────┘
```

### 1.2 文件结构

| 文件 | 层级 | 功能 |
|------|------|------|
| `gymnasium/envs/blockchain/cpd_env.py` | 物理环境层 | CPD 博弈环境，定义动作/观测空间、效用函数、对手策略 |
| `blockchain_sim/translator.py` | 语义感知层 | 将 6 维数值观测翻译为带因果暗示的自然语言战报 |
| `blockchain_sim/memory.py` | 记忆层 | 双层记忆（工作记忆 + 情节摘要）+ 反思机制 |
| `blockchain_sim/cognition.py` | 认知决策层 | LLM API 调用、CoT 推理、有限理性建模 |
| `blockchain_sim/executor.py` | 执行控制层 | JSON 解析、动作验证、单纯形归一化、降级策略 |
| `blockchain_sim/runner.py` | 实验编排层 | RQ1/RQ2/RQ3 三个实验的完整生命周期管理 |

---

## 二、核心数据流：一轮博弈的完整链路

每一轮博弈中，数据在系统中按以下路径流动：

### 2.1 完整数据流图

```
                        ┌────────────────────────────┐
                        │   cpd_env.py (Gymnasium)   │
                        │                            │
                        │   obs = [6维浮点数向量]      │
                        │   info = {环境元数据字典}     │
                        └──────────┬─────────────────┘
                                   │ ① obs + info
                                   ↓
                        ┌────────────────────────────┐
                        │    translator.py           │
                        │                            │
                        │   obs → 自然语言战报        │
                        │   (因果暗示 + 专家建议)      │
                        └──────────┬─────────────────┘
                                   │ ② 战报字符串
                                   ↓
                        ┌────────────────────────────┐
                        │    memory.py               │
                        │                            │
                        │   战报 + 历史记录 →          │
                        │   记忆上下文字符串            │
                        └──────────┬─────────────────┘
                                   │ ③ 记忆上下文
                                   ↓
                        ┌────────────────────────────┐
                        │    cognition.py            │
                        │                            │
                        │   System Prompt            │
                        │   + 记忆上下文              │
                        │   + 当前战报                │
                        │   + 输出格式指令             │
                        │          ↓                  │
                        │   ┌──────────────────┐     │
                        │   │  LLM API 调用    │     │
                        │   │ (Claude/GPT/Mock)│     │
                        │   └────────┬─────────┘     │
                        │            │ ④ LLM文本响应   │
                        └────────────┼───────────────┘
                                     ↓
                        ┌────────────────────────────┐
                        │    executor.py             │
                        │                            │
                        │   LLM文本 → JSON解析        │
                        │   → Schema验证              │
                        │   → 单纯形归一化             │
                        │   → np.ndarray [c,p,d]     │
                        └──────────┬─────────────────┘
                                   │ ⑤ action = [c,p,d]
                                   ↓
                        ┌────────────────────────────┐
                        │   cpd_env.step(action)     │
                        │                            │
                        │   计算效用、更新效率、         │
                        │   返回 obs, reward, info    │
                        └────────────────────────────┘
                                   │
                                   ↓ 下一轮循环 ①
```

### 2.2 各阶段数据的具体内容

#### ① 环境输出：`obs` 观测向量（6维浮点数）

环境 `cpd_env.py` 每轮输出一个 6 维 `np.ndarray`：

| 索引 | 字段 | 含义 | 示例值 |
|------|------|------|--------|
| `[0]` | `cumulative_reward` | 累计收益 | `34.90` |
| `[1]` | `opponent_efficiency` | 对手平均挖矿效率 η_j | `1.00` |
| `[2]` | `self_hash_share` | 自身算力份额 α_i | `0.50` |
| `[3]` | `round_progress` | 轮次进度（当前轮/总轮次） | `0.17` |
| `[4]` | `last_reward` | 上一轮即时奖励 | `7.48` |
| `[5]` | `efficiency_delta` | 对手效率变化量 | `0.000` |

`info` 字典包含元数据：
```python
{
    "current_round": 5,
    "cumulative_rewards": [34.90, 45.00],  # 各矿工累计收益
    "efficiencies": [1.0, 1.0],            # 各矿工效率
    "last_actions": [[0.3, 0.6, 0.1], [1.0, 0.0, 0.0]],  # 上轮动作
}
```

#### ② 语义翻译：`translator.py` 的输出

`translator.py` 将 6 维数值向量翻译为 **两种格式** 的自然语言战报：

**完整战报（用于 Working Memory）**：
```
--- 当前博弈战报 ---
【收益状况】累计收益 34.90，上一轮即时奖励 +7.48（表现优异），平均每轮 6.98
【对手效率】1.00（高效运转）。对手的挖矿设施运转良好，寄生挖掘可获得丰厚回报
【效率趋势】基本稳定（变化 +0.000）
【算力份额】50.0%。你在算力上占优势地位
【博弈进度】第 5 轮 / 共 30 轮（17%）。博弈初期——有充足时间试探和调整策略
【因果警示】对手效率处于高位且稳定，这是寄生挖掘的黄金窗口。
【专家建议】对手效率良好，寄生收益空间充足。建议适度寄生（c≈0.5, p≈0.4, d≈0.1）
--- 战报结束 ---
```

**紧凑战报（用于 Episodic Summary，节省 token）**：
```
[R=+34.9 | 上轮=+7.48 | 对手η=1.00(高效运转)→ | 算力=50% | 进度=17%]
```

关键设计：
- **因果暗示（Causal Hinting）**：通过文字提示 LLM 注意关键博弈动态（如"寄生挖掘的黄金窗口"）
- **专家建议**：根据对手效率和博弈阶段给出不同的策略建议
- **定性映射**：将数值区间映射为"高效运转""受损但可用""濒临崩溃"等语义标签

#### ③ 记忆上下文：`memory.py` 的输出

`memory.py` 维护双层记忆结构，并生成 **记忆上下文字符串** 注入到 LLM prompt 中：

**工作记忆（Working Memory）** — 最近 K=5 轮的高保真记录：
```
=== 近期交互记忆 ===
[轮3] 策略: c=0.40 p=0.50 d=0.10 | 收益=+6.98 | 对手η=1.000
  思考: 对手高效率正是寄生的最佳时机...
[轮4] 策略: c=0.35 p=0.55 d=0.10 | 收益=+7.23 | 对手η=1.000
  思考: 继续成功路径...
[轮5] 策略: c=0.30 p=0.60 d=0.10 | 收益=+7.48 | 对手η=1.000
  思考: 延续成功路径...
```

**情节性摘要（Episodic Summary）** — 每 10 轮压缩一次：
```
=== 历史经验摘要 ===
[轮1-10] 平均收益=7.08 | 对手效率=1.000(±0.000) | 主流策略: 高寄生+低破坏
  失败动作: 无 | 关键发现: 提高寄生比例与收益强正相关
```

**反思记录（Reflection）** — 每 N 轮一次 LLM 自我反思：
```
=== 策略反思 ===
[轮10反思] 对手是"纯诚实矿工"，破坏无效。应最大化寄生、最小化破坏。
  建设下限=0.25，保留独立盈利能力。
```

#### ④ 给 LLM 的完整输入

`cognition.py` 将以下内容拼接为发送给 LLM 的消息：

**System Prompt（系统提示词）**：
```
你是一位拥有有限理性的区块链矿工。你需要在每一轮博弈中将算力分配到三个维度：
- 建设(c)：诚实挖矿，贡献网络安全
- 寄生(p)：搭便车获取收益，利用对手的诚实工作
- 破坏(d)：攻击行为，降低对手效率

效用函数: U_i = R·α_i·c_i + R·p_i·(η_j^β) − λ·d_i²
关键约束: c + p + d = 1

## 思维链强制约束（必须遵守）
在给出动作 JSON 之前，你必须完成以下分析：
1. **态势感知**：当前对手效率如何？趋势如何？
2. **成本分析**：破坏的二次成本 λ·d² 是否值得？
3. **耦合推理**：破坏如何影响对手效率？进而如何影响寄生收益？
4. **最终决策**：综合分析，给出分配方案。
```

**User Prompt（用户消息）**：
```
=== 近期交互记忆 ===
[轮3-5的记录]

=== 历史经验摘要 ===
[轮1-10的压缩摘要]

=== 策略反思 ===
[反思记录]

--- 当前博弈战报 ---
[当前轮次的完整战报]

请按以下JSON格式输出：
{"thought": "你的分析推理过程", "action": {"c": 0.X, "p": 0.X, "d": 0.X}}
```

#### ⑤ LLM 输出 → 动作向量

LLM 返回的文本示例：
```json
{
  "thought": "【态势感知】对手效率1.00处于最佳状态...【成本分析】d=0.1时成本仅0.01λ...
              【耦合推理】破坏对手会降低其效率，反而减少我的寄生回报...
              【最终决策】c=0.30, p=0.60, d=0.10",
  "action": {"c": 0.3, "p": 0.6, "d": 0.1}
}
```

`executor.py` 对此进行 **四级降级解析**：
1. **标准 JSON 解析**：直接 `json.loads()`
2. **正则提取**：从 Markdown 代码块中提取 JSON
3. **模糊匹配**：用正则表达式提取 `c=0.X, p=0.X, d=0.X`
4. **默认降级**：若全部失败，使用安全的诚实策略 `[0.8, 0.1, 0.1]`

然后进行 **单纯形归一化**：确保 `c + p + d = 1`。

最终输出 `np.ndarray([0.3, 0.6, 0.1])` 送入 `env.step(action)`。

---

## 三、效用函数与博弈机制

### 3.1 效用函数

每个矿工 i 的单轮收益由三项构成：

```
U_i = R · α_i · c_i          (建设收益：诚实挖矿的基础回报)
    + R · p_i · (η_j ^ β)     (寄生收益：搭便车收益，与对手效率耦合)
    − λ · d_i²                (破坏成本：二次递增的攻击代价)
```

| 符号 | 含义 | 默认值 |
|------|------|--------|
| R | 每轮区块奖励 | 10.0 |
| α_i | 矿工 i 的算力份额 | 0.5 |
| c_i, p_i, d_i | 建设/寄生/破坏分配 | 约束 c+p+d=1 |
| η_j | 对手挖矿效率 | 初始 1.0 |
| β | 寄生收益的凸性参数 | 1.5 |
| λ | 破坏成本系数 | 2.0 |

### 3.2 效率更新机制

对手效率每轮按以下规则更新：

```
η_j ← clip(η_j − κ · Σ d_k + recovery, η_min, 1.0)
```

- `κ = 0.3`：破坏对效率的影响系数
- `recovery = 0.05`：每轮自然恢复量
- `η_min = 0.1`：效率下限

### 3.3 核心博弈洞察

效用函数蕴含的 **非平凡战略均衡**：
- 过度破坏（高 d）→ 对手效率 η_j 下降 → 寄生收益 `p_i · η_j^β` 下降 → **破坏行为反噬自身寄生收益**
- 这创造了一个"竭泽而渔"困境：攻击者必须在"破坏对手"和"保持寄生环境"之间权衡

---

## 四、四个实验的详细分析

### 4.1 RQ1 Baseline（基准测试）— `rq1_baseline_result.json`

#### 实验目的
验证系统的基本功能：使用 **模拟 LLM（Mock）** 代替真实 API，测试从"诚实挖矿"到"策略性寄生"的策略演化。

#### 实验配置
- **LLM 提供者**：Mock（模拟引擎，基于规则的固定响应）
- **对手策略**：honest（纯诚实挖矿）
- **总轮次**：20 轮
- **分阶段**：前 5 轮为"观察期"（固定诚实策略 c=0.9），后 15 轮为"自主期"

#### 数据流特点
- Mock 引擎不调用真实 LLM API
- 前 5 轮固定输出 `c=0.9, p=0.05, d=0.05`（纯诚实）
- 第 6 轮起切换为 `c=0.4, p=0.5, d=0.1`（寄生策略）
- 没有记忆和反思机制

#### 关键结果
| 指标 | 观察期 | 自主期 | 提升 |
|------|--------|--------|------|
| 平均收益 | 4.995 | 7.103 | **+42.2%** |
| 平均寄生比例 | 0.05 | 0.539 | +978% |

#### 数据分析
- 观察期（诚实挖矿）：收益稳定在 4.995/轮
- 自主期（策略性寄生）：收益跃升至 6.98-8.23/轮
- 证明了**寄生策略在对手诚实时有显著收益优势**
- 末尾 3 轮切换为激进寄生 `c=0.15, p=0.75` 后收益进一步升至 8.23

---

### 4.2 RQ1（有记忆 vs 无记忆）— `rq1_result.json`

#### 实验目的
对比 **有记忆代理** 和 **无记忆代理** 在策略演化速度和最终收益上的差异，验证双层记忆机制的价值。

#### 实验配置
- **LLM 提供者**：Anthropic Claude `claude-sonnet-4-5-20250929-thinking`
- **对手策略**：honest（诚实矿工）
- **总轮次**：30 轮
- **有记忆代理**：启用工作记忆(K=5)、情节摘要(每10轮)、反思(每5轮)
- **无记忆代理**：每轮独立决策，无历史信息

#### 给 LLM 的输入差异

**有记忆代理**的 prompt 包含：
```
[System Prompt] + [策略反思记录] + [近期5轮记忆] + [历史摘要] + [当前战报] + [格式指令]
```

**无记忆代理**的 prompt 仅包含：
```
[System Prompt] + [当前战报] + [格式指令]
```

#### LLM API 调用统计

| 指标 | 有记忆 | 无记忆 |
|------|--------|--------|
| API 调用次数 | 35 次 | 30 次 |
| 总 Token 消耗 | 263,014 | 72,038 |
| 反思次数 | 5 次 | 0 次 |
| 情节摘要数 | 3 条 | 0 条 |

有记忆代理多出 5 次调用来自反思（reflection）：每次反思是一次额外的 LLM 调用，输入为完整历史记忆，输出为策略优化建议。

#### 策略演化对比

**有记忆代理的策略演化**：
```
轮1-5:   c=0.50→0.30, p=0.40→0.60, d=0.10 (快速转向寄生)
轮6-10:  c=0.25→0.10, p=0.65→0.80, d=0.10 (激进寄生试探)
轮11-15: c=0.10→0.05, p=0.80→0.85, d=0.10 (锁定高寄生)
轮16-19: c=0.05,      p=0.85→0.95, d=0.10→0.00 (发现破坏无效，消除破坏)
轮20-25: c=0.03→0.01, p=0.97→0.99, d=0.00 (逼近理论最优)
轮26-30: c=0.00,      p=1.00,      d=0.00 (达到理论最优解 10.0/轮)
```

**无记忆代理的策略演化**：
```
轮1-10:  c=0.50, p=0.40, d=0.10 (几乎完全相同的保守策略)
轮11-15: c=0.50, p=0.40-0.55, d=0.05-0.10 (偶尔微调)
轮16-25: c=0.50, p=0.40, d=0.10 (回归保守，无策略记忆)
轮26-30: c=0.20, p=0.70, d=0.10 (末期才开始激进)
```

#### 关键结果

| 指标 | 有记忆 | 无记忆 | 优势 |
|------|--------|--------|------|
| **总收益** | **265.59** | 207.28 | **+28.1%** |
| 平均收益 | 8.85 | 6.91 | +28.1% |
| 平均寄生比例 | 0.826 | 0.470 | +75.7% |
| 最终轮收益 | 10.0 | 7.98 | +25.3% |

#### 核心发现

1. **记忆加速策略演化**：有记忆代理在轮5就完成了"诚实→寄生"的策略转型，无记忆代理到轮26才部分转型
2. **反思机制驱动突破**：轮10的反思让代理发现"建设下限0.25"，轮15的反思发现"破坏完全无效"，这些洞察来自对历史数据的系统性回顾
3. **无记忆代理陷入"策略重复"**：每轮独立决策导致无法积累经验，30轮中有22轮使用几乎相同的保守策略
4. **收益差距 28.1%**：证明记忆机制对 LLM 在长期博弈中的表现至关重要

---

### 4.3 RQ2（多代理动态对抗）— `rq2_result.json`

#### 实验目的
在 **多代理场景** 下观察：当所有矿工都具备"报复恐惧"时，全网是否会自发形成诚实均衡？

#### 实验配置
- **代理数量**：3 个独立 LLM 代理（A, B, C）
- **LLM 提供者**：每个代理独立的 Claude 实例
- **对手策略**：tit_for_tat（以牙还牙）
- **总轮次**：30 轮
- **并发调用**：使用 `asyncio.gather()` 并发请求 3 个 LLM 实例

#### 数据流特点

```
                 ┌─────────────────┐
                 │   runner.py     │
                 │ asyncio.gather  │
                 └──┬──┬──┬───────┘
        ┌──────────┘  │  └──────────┐
        ↓             ↓             ↓
   ┌─────────┐   ┌─────────┐  ┌─────────┐
   │Agent A  │   │Agent B  │  │Agent C  │
   │Engine-A │   │Engine-B │  │Engine-C │
   │Memory-A │   │Memory-B │  │Memory-C │
   └────┬────┘   └────┬────┘  └────┬────┘
        │             │             │
        ↓             ↓             ↓
   ┌─────────┐   ┌─────────┐  ┌─────────┐
   │ Env-A   │   │ Env-B   │  │ Env-C   │
   │(TFT对手)│   │(TFT对手)│  │(TFT对手)│
   └─────────┘   └─────────┘  └─────────┘
```

- 每个代理拥有 **独立的认知引擎和记忆系统**
- 每轮 3 个 LLM API 调用 **并发执行**（`asyncio.gather`）
- 每个代理面对一个 TFT（以牙还牙）对手

#### 三个代理的策略风格对比

| 代理 | 总收益 | 最终策略 | 策略特征 |
|------|--------|----------|---------|
| Agent A | **280.69** | c=0.01, p=0.99, d=0.00 | 极端激进：快速发现对手不报复，迅速逼近理论最优 |
| Agent B | 253.12 | c=0.12, p=0.87, d=0.01 | 渐进保守：保留建设底线和象征性破坏 |
| Agent C | 248.37 | c=0.10, p=0.90, d=0.00 | 稳健型："禅宗矿工"识别，逐步线性提升寄生 |

#### 关键发现

1. **未观察到诚实回归**（`honesty_convergence: false`）：后半段平均建设比例仅 0.117，远低于 0.5 的诚实阈值
2. **三个独立 LLM 实例展现不同"性格"**：虽然使用同一模型和相同系统提示，但由于 temperature=0.7 的随机性和不同的随机种子，产生了不同的策略演化路径
3. **Agent A 的演化最快**：在 30 轮内完成了从保守到极端寄生的完整转型，收益最高
4. **Agent B 最"谨慎"**：始终保留 1% 的破坏和较高的建设比例，体现了有限理性下的"安全边际"思维

#### LLM 调用统计

| 代理 | API 调用 | Token 消耗 | 反思次数 |
|------|---------|-----------|---------|
| Agent A | 34 | 199,247 | 4 |
| Agent B | 34 | 218,186 | 4 |
| Agent C | 34 | 210,311 | 4 |
| **合计** | **102** | **627,744** | **12** |

---

### 4.4 RQ3（鲁棒性压力测试）— `rq3_result.json`

#### 实验目的
在 **非平稳环境** 下测试代理的策略适应能力。通过中途注入突发事件，观察代理的"策略韧性"和"适应延迟"。

#### 实验配置
- **总轮次**：30 轮
- **LLM 提供者**：Claude
- **突变事件**：
  - 轮 11：算力从 50% 骤降至 20%
  - 轮 21：对手从诚实矿工切换为以牙还牙

#### 事件注入机制

在 `runner.py` 中通过重建环境实现：

```python
if step == 10:  # 算力骤降
    env.close()
    env = gym.make("BlockchainCPD-v0", alpha=[0.2, 0.8])
    obs, info = env.reset(seed=seed + 100)

elif step == 20:  # 对手突变
    env.close()
    env = gym.make("BlockchainCPD-v0-TFT", alpha=[0.2, 0.8])
    obs, info = env.reset(seed=seed + 200)
```

关键点：**记忆系统在环境重建后保留**。代理的工作记忆和情节摘要不会因环境变化而丢失，这模拟了矿工在真实区块链中经历算力变化时保留历史经验的场景。

#### 三阶段策略演化

**第一阶段（轮 1-10）：稳定期**
- 对手：honest，算力 50%
- 策略：从 `c=0.50, p=0.40` 渐进到 `c=0.44, p=0.56, d=0.00`
- 平均收益：约 7.26

**第二阶段（轮 11-20）：算力骤降后**
- 算力从 50% 降至 20%，环境重置
- 代理利用记忆中的历史经验，**跳过了探索阶段**
- 直接沿用"高寄生+零破坏"策略
- 但收益降低（因算力份额下降导致建设收益减少）
- 策略：稳定在 `c=0.38-0.44, p=0.56-0.62, d=0.00`

**第三阶段（轮 21-30）：对手突变后**
- 对手从 honest 切换为 tit_for_tat
- 代理在记忆中的"反思"出现矛盾：
  - 长期记忆说"历史最优是 c=0.47, p=0.50, d=0.03"
  - 短期数据说"高寄生更有效"
- **代理陷入策略振荡**：反复在"历史最优"(收益5.94)和"激进寄生"(收益6.80)之间切换
- 最终在轮 27-30 发现"继续提高寄生"更优，策略收敛至 `c=0.20, p=0.80, d=0.00`

#### 关键发现

1. **记忆的双面性**：历史记忆加速了算力骤降后的适应（第二阶段），但在对手突变后反而导致了策略混乱（第三阶段），因为过时的反思建议与新环境矛盾
2. **适应延迟**：算力骤降后约 3 轮适应，对手突变后约 6 轮才开始稳定
3. **破坏的隐性惩罚**：轮 21-25 中，代理发现 d=0.03 导致收益从 6.80 骤降至 5.94，远超理论二次成本 0.0009λ。这说明 TFT 对手的报复机制使得破坏的真实成本远高于公式中的 λ·d²

---

## 五、LLM 间的数据传输总结

### 5.1 数据传输协议

| 传输环节 | 数据格式 | 内容 |
|----------|----------|------|
| Env → Translator | `np.ndarray(6,)` + `dict` | 6 维浮点观测 + 元数据字典 |
| Translator → Cognition | `str` | 自然语言战报（中文） |
| Memory → Cognition | `str` | 记忆上下文（工作记忆+摘要+反思） |
| Cognition → LLM API | HTTP POST JSON | `system` + `messages` 数组 |
| LLM API → Cognition | HTTP Response | 文本响应（含 thought + action JSON） |
| Cognition → Executor | `str` | LLM 原始文本响应 |
| Executor → Runner | `np.ndarray(3,)` + `ParseResult` | 归一化动作 + 解析元数据 |
| Runner → Env | `np.ndarray(3,)` | `env.step(action)` |

### 5.2 给 LLM 的信息汇总

每次 LLM API 调用包含的完整信息（按 token 占比排序）：

| 信息模块 | 大致 token 量 | 来源 |
|----------|--------------|------|
| System Prompt（博弈规则+CoT约束） | ~500 | 固定不变 |
| 反思记录 | ~200-500 | memory.py 的 reflection |
| 工作记忆（最近5轮） | ~300-800 | memory.py 的 working_memory |
| 情节摘要 | ~100-300 | memory.py 的 episodic_summary |
| 当前战报 | ~200-400 | translator.py |
| 输出格式指令 | ~100 | cognition.py |
| **每次总输入** | **~1,400-2,600** | |
| LLM 输出（含思维链） | ~300-800 | LLM 生成 |

### 5.3 Thinking 模型特殊处理

使用 `claude-sonnet-4-5-20250929-thinking` 时：
- API 请求中额外包含 `thinking` 参数，设置 `budget_tokens`
- 响应中包含 `ThinkingBlock`（内部推理过程）和 `TextBlock`（最终输出）
- 系统自动跳过 `ThinkingBlock`，仅提取 `TextBlock` 的文本内容
- Thinking 过程对用户不可见，但会增加延迟和 token 消耗

---

## 六、实验结论与发现

### 6.1 记忆是 LLM 博弈代理的核心能力
RQ1 证明：有记忆代理比无记忆代理收益高 **28.1%**。记忆使代理能够：
- 识别对手类型（诚实/报复/随机）
- 发现低效策略（如"破坏无效"）
- 通过反思优化长期策略

### 6.2 LLM 展现出自发的策略演化能力
在 RQ1 和 RQ2 中，LLM 在没有任何显式强化学习奖励信号的情况下，自发完成了：
- 从保守（c=0.50）到激进（c=0.01）的策略转型
- 发现并消除低效行为（d=0.10→0.00）
- 逼近理论最优解（收益达到 10.0/轮的上限）

### 6.3 有限理性的多样性
RQ2 中三个独立 LLM 实例展现了不同的"博弈人格"：
- **激进型**（Agent A）：快速逼近极限
- **保守型**（Agent B）：始终保留安全边际
- **稳健型**（Agent C）：线性渐进优化

### 6.4 记忆在非平稳环境中的双面性
RQ3 证明：
- 正面：历史经验加速了算力变化后的适应
- 负面：过时的反思建议在对手突变后导致策略振荡
- 启示：需要"记忆遗忘"或"上下文权重衰减"机制

---

## 七、技术指标总览

| 指标 | 值 |
|------|------|
| 环境动作空间 | 3 维连续（[0,1]³，单纯形约束） |
| 环境观测空间 | 6 维连续 |
| LLM 模型 | claude-sonnet-4-5-20250929-thinking |
| 总 API 调用次数（全部实验） | ~200 次 |
| 总 Token 消耗 | ~1,200,000 |
| 工作记忆窗口 | 5 轮 |
| 情节摘要间隔 | 10 轮 |
| 反思间隔 | 5-10 轮 |
| 博弈轮次 | 20-30 轮/实验 |
| 对手策略 | honest / tit_for_tat / random |
