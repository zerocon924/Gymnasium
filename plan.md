

## 实验二（RQ2）重构指南：多智能体联合博弈环境

### 1. 建模范式转移：从 MDP 升级为 POMG

目前的实验是单代理马尔可夫决策过程（MDP），需升级为**部分观测马尔可夫博弈（Partially Observed Markov Game, POMG）** 。

* 
**统一环境逻辑**：构造一个中心化的 `MultiAgentBlockchainEnv`（基于 Gymnasium），支持三个代理节点  同场竞技 。


* 
**状态互感（Inter-observability）**：在一个代理的 `observation` 中，必须包含其他代理的公开状态（如：累计收益、当前挖掘效率 、过去的策略倾向标签） 。


* 
**动态耦合效应**：代理  发起的破坏动作 ，其产生的反噬效应（效率下降 ）必须实时反映在  和  的下一轮观测中 。



### 2. 算力份额 () 的重新设定

为模拟真实公链环境，算力分配需打破“单打独斗”的局限 ：

* **非饱和分配规则**：设定 。
* 例如：。
* 
**背景诚实算力**：剩余的 （如 0.4）由环境中的“背景诚实节点集群（Honest Group）”占据 。




* 
**意义**：这样确保了代理不仅在与彼此竞争，还在与一个代表协议底线的“诚实群体”博弈，更符合 SquirRL 第六章的  设定 。



### 3. 独立认知层与私有记忆机制

每个代理必须拥有完全隔离的认知架构，严禁共享 API Context ：

* 
**独立记忆模块 (`Memory_i`)**：每个节点维护自己的 `long_term_memory`（存储历史战报）和 `reflection_log`（存储对自己策略的反思） 。


* 
**个性化 CoT (思维链)**：在每一轮，三个智能体需串行或并发生成独立的逻辑推导（如  判定  为“激进者”，从而决定增加  以对冲风险） 。



### 4. 操作顺序与翻译层（Translator）升级

* 
**同步决策机制**：参考 SquirRL 的执行规则，所有代理在看到上一轮“全网状态”后同步提交 CPD 向量 。


* **语义标签化描述**：升级 `translator.py`。观测数据不再只是自己的数值，需包含**社会性描述**：
> “当前全网有 3 个竞争矿工。Agent 0 正在发动攻击，导致你的效率  下降。Agent 1 表现诚实。背景诚实算力稳健。”



### 5. 核心评估指标：寄生者共识（Parasitic Equilibrium）

我们需要观测并记录以下现象：

* 
**社会性标签演化**：记录 LLM 在推理过程中给对手贴的标签（如“禅宗矿工”、“剥削者”） 。


* 
**收敛趋势**：观察三个代理是否会在博弈中后期达成“互不破坏、共同剥削诚实算力”的坏均衡（即  但  极高） 。



---

### 给程序员的技术提示：

1. **代码结构**：请修改 `main.py` 的循环逻辑，从 `for i in range(3): run_single_agent()` 更改为 `while not done: env.step({a0: act, a1: act, a2: act})`。
2. **API 调用**：虽然是 3 个代理，但为了模拟“博弈感”，请确保 Prompt 能够体现出“你是节点 ，你要在其他两人存在的情况下做决定”。
3. **算力分配**：请在配置文件中将 `total_hashpower` 设置为 1.0，并分配 0.4 给背景 Honest 进程，其余 0.6 由 3 个 LLM 代理分担。
